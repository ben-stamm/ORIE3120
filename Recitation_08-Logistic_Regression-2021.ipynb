{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M_sj9n9iJwxh"
   },
   "source": [
    "# Recitation 8: Logistic Regression\n",
    "\n",
    "This recitation will be due with the homework on Monday April 12 at 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xKP8Pq4mYyM_"
   },
   "source": [
    "## Case Background\n",
    "In this lab, we continue the analysis of the metal casting problem from the previous recitation. Recall that, as part of a longer\n",
    "manufacturing process, a factory needs to cast a large number of rectangular metallic blocks.\n",
    "The blocks are manufactured using a mold, consisting of the main cavity, a cup through\n",
    "which the molten metal is poured, and two risers for cooling. The size and shape of the\n",
    "pouring cup and risers affect how quickly the metal can be poured into the mold, how\n",
    "quickly it cools, and whether it sets correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dh9soj6lKB9_"
   },
   "source": [
    "## Objective\n",
    "The factory needs to cast batches of 100 blocks of size 4.5×4.5×7 inches. The\n",
    "current casting approach is conservative – it takes a long time to pour, but the blocks\n",
    "always set correctly and are usable. Your goal is to achieve a significant reduction in\n",
    "average casting time while still ensuring that most blocks are usable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vUX8BCsRHoxe"
   },
   "source": [
    "## Variables describing the mold\n",
    "\n",
    "The following nine variables describe the shape of the mold:\n",
    "* Riser Height\n",
    "* Riser Diameter\n",
    "* Riser 1 Position\n",
    "* Riser 2 Position\n",
    "* Gate Diameter\n",
    "* Cup Height\n",
    "* Sprue Height\n",
    "* Sprue Diameter Bottom\n",
    "* Sprue Diameter Top\n",
    "Together we call these the \"mold variables\".\n",
    "\n",
    "We will vary these parameters to choose a new shape for the mold to reduce average casting time while ensuring most blocks are usable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yb2ZzEpLJey_"
   },
   "source": [
    "## Data\n",
    "\n",
    "To obtain data on how the mold variables affect pouring and cooling, a batch of 100 blocks are poured while randomly varying the nine mold variables around their baseline values. The data is available in castdata.csv. This is the same file as the previous recitation. As before, the first nine columns are the mold-variables listed above. The 10th column is `BatchTime` (how long the molten metal took to cool inside the mold). The 11th column is `Feasible`. `Feasible` is 1 if the block that came out of the mold was usable and 0 if it wasn't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "C5fjXrHwJlrQ"
   },
   "source": [
    "## ASSIGNMENT\n",
    "\n",
    "This lab project will guide you through the analysis. First, we must import castdata.csv into the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CoP3YPMlJj1y"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GateDiam</th>\n",
       "      <th>CupHeight</th>\n",
       "      <th>SprueHeight</th>\n",
       "      <th>RiserDiam</th>\n",
       "      <th>SprueDiamBot</th>\n",
       "      <th>SprueDiamTop</th>\n",
       "      <th>RiserHeight</th>\n",
       "      <th>Riser1Pos</th>\n",
       "      <th>Riser2Pos</th>\n",
       "      <th>BatchTime</th>\n",
       "      <th>Feasible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3031.292318</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.318317</td>\n",
       "      <td>5.604768</td>\n",
       "      <td>10.148839</td>\n",
       "      <td>6.745247</td>\n",
       "      <td>0.635632</td>\n",
       "      <td>0.889822</td>\n",
       "      <td>5.318354</td>\n",
       "      <td>3.168774</td>\n",
       "      <td>5.233322</td>\n",
       "      <td>3042.417820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.353834</td>\n",
       "      <td>5.846289</td>\n",
       "      <td>9.528183</td>\n",
       "      <td>7.345273</td>\n",
       "      <td>0.642489</td>\n",
       "      <td>1.005683</td>\n",
       "      <td>4.836570</td>\n",
       "      <td>3.265961</td>\n",
       "      <td>5.482437</td>\n",
       "      <td>3050.300487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.596163</td>\n",
       "      <td>4.280124</td>\n",
       "      <td>10.887751</td>\n",
       "      <td>6.952856</td>\n",
       "      <td>0.317180</td>\n",
       "      <td>0.589558</td>\n",
       "      <td>5.492964</td>\n",
       "      <td>4.029533</td>\n",
       "      <td>4.719791</td>\n",
       "      <td>3034.202547</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.484338</td>\n",
       "      <td>5.205026</td>\n",
       "      <td>11.003688</td>\n",
       "      <td>7.254450</td>\n",
       "      <td>0.531424</td>\n",
       "      <td>0.595769</td>\n",
       "      <td>4.897376</td>\n",
       "      <td>4.011947</td>\n",
       "      <td>4.822747</td>\n",
       "      <td>3033.046509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GateDiam  CupHeight  SprueHeight  RiserDiam  SprueDiamBot  SprueDiamTop  \\\n",
       "0  0.500000   5.000000    10.000000   7.000000      0.500000      0.750000   \n",
       "1  0.318317   5.604768    10.148839   6.745247      0.635632      0.889822   \n",
       "2  0.353834   5.846289     9.528183   7.345273      0.642489      1.005683   \n",
       "3  0.596163   4.280124    10.887751   6.952856      0.317180      0.589558   \n",
       "4  0.484338   5.205026    11.003688   7.254450      0.531424      0.595769   \n",
       "\n",
       "   RiserHeight  Riser1Pos  Riser2Pos    BatchTime  Feasible  \n",
       "0     5.000000   3.500000   5.000000  3031.292318         1  \n",
       "1     5.318354   3.168774   5.233322  3042.417820         0  \n",
       "2     4.836570   3.265961   5.482437  3050.300487         1  \n",
       "3     5.492964   4.029533   4.719791  3034.202547         1  \n",
       "4     4.897376   4.011947   4.822747  3033.046509         1  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "df = pd.read_csv('castdata.csv')\n",
    "df.head() # view the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EiNj4SFr0QEy"
   },
   "source": [
    "Now we compare the distributions of the predictors (the mold variables) among two subsets of the data: those with Feasible = 0 and those with Feasible = 1. \n",
    "\n",
    "Before doing this, we first normalize each of the predictors by subtracting off its mean value and dividing by its standard deviation. This puts all of the predictors on the same scale, so that we can plot them together. (The boxplot() function we use below doesn't allow multiple y axes, so if we didn't do this then we would mostly see that the different predictors have different mean values, which wouldn't be very informative.) Also, by subtracting off the mean value and dividing by their standard deviation, we can more easily see how changes in these values, within the range evaluated in our data, affect feasibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GateDiam</th>\n",
       "      <th>CupHeight</th>\n",
       "      <th>SprueHeight</th>\n",
       "      <th>RiserDiam</th>\n",
       "      <th>SprueDiamBot</th>\n",
       "      <th>SprueDiamTop</th>\n",
       "      <th>RiserHeight</th>\n",
       "      <th>Riser1Pos</th>\n",
       "      <th>Riser2Pos</th>\n",
       "      <th>BatchTime</th>\n",
       "      <th>Feasible</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.093566</td>\n",
       "      <td>-0.038859</td>\n",
       "      <td>-0.112868</td>\n",
       "      <td>0.037250</td>\n",
       "      <td>-0.051764</td>\n",
       "      <td>-0.078771</td>\n",
       "      <td>0.091616</td>\n",
       "      <td>0.024610</td>\n",
       "      <td>0.149696</td>\n",
       "      <td>3031.292318</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.161374</td>\n",
       "      <td>1.499244</td>\n",
       "      <td>0.155546</td>\n",
       "      <td>-0.428747</td>\n",
       "      <td>1.055643</td>\n",
       "      <td>0.698712</td>\n",
       "      <td>0.818235</td>\n",
       "      <td>-0.855144</td>\n",
       "      <td>0.653394</td>\n",
       "      <td>3042.417820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.916050</td>\n",
       "      <td>2.113506</td>\n",
       "      <td>-0.963736</td>\n",
       "      <td>0.668825</td>\n",
       "      <td>1.111626</td>\n",
       "      <td>1.342958</td>\n",
       "      <td>-0.281401</td>\n",
       "      <td>-0.597010</td>\n",
       "      <td>1.191190</td>\n",
       "      <td>3050.300487</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.757796</td>\n",
       "      <td>-1.869717</td>\n",
       "      <td>1.488091</td>\n",
       "      <td>-0.048987</td>\n",
       "      <td>-1.544448</td>\n",
       "      <td>-0.970910</td>\n",
       "      <td>1.216770</td>\n",
       "      <td>1.431078</td>\n",
       "      <td>-0.455224</td>\n",
       "      <td>3034.202547</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.014619</td>\n",
       "      <td>0.482582</td>\n",
       "      <td>1.697170</td>\n",
       "      <td>0.502691</td>\n",
       "      <td>0.204806</td>\n",
       "      <td>-0.936376</td>\n",
       "      <td>-0.142616</td>\n",
       "      <td>1.384370</td>\n",
       "      <td>-0.232962</td>\n",
       "      <td>3033.046509</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GateDiam  CupHeight  SprueHeight  RiserDiam  SprueDiamBot  SprueDiamTop  \\\n",
       "0  0.093566  -0.038859    -0.112868   0.037250     -0.051764     -0.078771   \n",
       "1 -1.161374   1.499244     0.155546  -0.428747      1.055643      0.698712   \n",
       "2 -0.916050   2.113506    -0.963736   0.668825      1.111626      1.342958   \n",
       "3  0.757796  -1.869717     1.488091  -0.048987     -1.544448     -0.970910   \n",
       "4 -0.014619   0.482582     1.697170   0.502691      0.204806     -0.936376   \n",
       "\n",
       "   RiserHeight  Riser1Pos  Riser2Pos    BatchTime  Feasible  \n",
       "0     0.091616   0.024610   0.149696  3031.292318         1  \n",
       "1     0.818235  -0.855144   0.653394  3042.417820         0  \n",
       "2    -0.281401  -0.597010   1.191190  3050.300487         1  \n",
       "3     1.216770   1.431078  -0.455224  3034.202547         1  \n",
       "4    -0.142616   1.384370  -0.232962  3033.046509         1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized = df.copy()\n",
    "predictors = np.delete(normalized.columns.values,[9,10]) # All columns except Feasible and BatchTime are predictors\n",
    "for i in predictors:\n",
    "    # Normalize all predictors\n",
    "    normalized[i]=(normalized[i]-normalized[i].mean())/normalized[i].std()\n",
    "    \n",
    "normalized.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make a boxplot showing the distribution of each predictor in the two subsets of the data (those with Feasible=0 and Feasible=1).\n",
    "\n",
    "If you haven't used boxplots before, here is how they work. A boxplot uses a box to visualize a collection of observations. In the center of the box is a green line. The height of this line is the median of the observations. The height of the top of the box is the 75% quantile (the number such that 75% of the observations are below it). The height of the bottom of the box is the 25% quantile. There are lines coming out of the box (called\n",
    "“whiskers”) that represent the range of values over which most of the data falls. There may be a few circles above and below the whiskers, which are data that fall outside of this range. These points are called “outliers”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x0000016A2D54EE48>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000016A2D85A978>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000016A2D88EBE0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000016A2D8C3E80>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x0000016A2D902160>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000016A2D933400>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000016A2D9686A0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000016A2D999978>],\n",
       "       [<matplotlib.axes._subplots.AxesSubplot object at 0x0000016A2D9999B0>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000016A2D9FFE80>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000016A2DA3A470>,\n",
       "        <matplotlib.axes._subplots.AxesSubplot object at 0x0000016A2DA6BA20>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x864 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "normalized.boxplot(column = predictors.tolist(), by = 'Feasible', layout=(3,4), figsize=(12,12))\n",
    "# layout=(2,2) configures how the plots will be outputted into the console\n",
    "# for more info on boxplot() head to the link:\n",
    "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.boxplot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQrOHSOtFZJB"
   },
   "source": [
    "Based on the boxplots above, we can get a rough sense for which variables have the biggest effect on feasibility. To do this, we can look at the boxes for a predictor to see how the quantiles (25%, 50%, 75%) differ between the two subsets: Feasible = 1 and Feasible = 0. If each of these quantiles is similar, so that the two boxes have the same top, green line, and bottom, then the predictor probably doesn't have much effect on Feasible. But if some or all of these aspects of the box are very different from each other, then the predictor likely has a bigger effect.\n",
    "\n",
    "\n",
    "# Question 1\n",
    "Based on the boxplots above, which two or three variables have the biggest difference on Feasible? For those variables, are they higher or lower on average when Feasible = 1 rather than 0? Use these observations to make an initial recommendation about values of the mold-variables that will maximize the probability of a feasible casting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pkWC5JzoAicr"
   },
   "source": [
    "Ans: RiserDiam, SpruceDiamBot, and SpruceDiamTop have the biggest difference on Feasible. RiserDiam is higher on average when feasible = 1, while SpruceDiamBot and SpruceDiamTop are lower on average when Feasible =1. The initial recommendation is to increase RiserDiam, decrease SpruceDiamBot, and decrease SpruceDiam top to maximize probability of a feasible casting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2crzbZWBsUM8"
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "We will fit a logistic regression model to estimate the probability that a casting is feasible\n",
    "using the mold-variables as predictors. Fit an initial logistic regression model, using the\n",
    "following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ii2BvNF2sYc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.293880\n",
      "         Iterations 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bls24\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2389: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
      "  return ptp(axis=axis, out=out, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Feasible</td>     <th>  No. Observations:  </th>  <td>   100</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    90</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     9</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 06 Apr 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.5699</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>15:17:39</td>     <th>  Log-Likelihood:    </th> <td> -29.388</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -68.331</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>4.243e-13</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>        <td>  -14.9324</td> <td>    9.737</td> <td>   -1.534</td> <td> 0.125</td> <td>  -34.017</td> <td>    4.152</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GateDiam</th>     <td>   -4.8123</td> <td>    2.709</td> <td>   -1.776</td> <td> 0.076</td> <td>  -10.123</td> <td>    0.498</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CupHeight</th>    <td>   -0.5460</td> <td>    0.867</td> <td>   -0.630</td> <td> 0.529</td> <td>   -2.245</td> <td>    1.153</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SprueHeight</th>  <td>   -0.2529</td> <td>    0.663</td> <td>   -0.382</td> <td> 0.703</td> <td>   -1.552</td> <td>    1.046</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RiserDiam</th>    <td>    5.1473</td> <td>    1.155</td> <td>    4.457</td> <td> 0.000</td> <td>    2.884</td> <td>    7.411</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SprueDiamBot</th> <td>  -13.7618</td> <td>    4.284</td> <td>   -3.213</td> <td> 0.001</td> <td>  -22.158</td> <td>   -5.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SprueDiamTop</th> <td>   -8.8910</td> <td>    2.445</td> <td>   -3.637</td> <td> 0.000</td> <td>  -13.683</td> <td>   -4.099</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RiserHeight</th>  <td>    0.5067</td> <td>    0.875</td> <td>    0.579</td> <td> 0.562</td> <td>   -1.208</td> <td>    2.221</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Riser1Pos</th>    <td>   -0.0719</td> <td>    0.839</td> <td>   -0.086</td> <td> 0.932</td> <td>   -1.717</td> <td>    1.573</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Riser2Pos</th>    <td>   -0.2144</td> <td>    0.766</td> <td>   -0.280</td> <td> 0.780</td> <td>   -1.716</td> <td>    1.287</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               Feasible   No. Observations:                  100\n",
       "Model:                          Logit   Df Residuals:                       90\n",
       "Method:                           MLE   Df Model:                            9\n",
       "Date:                Tue, 06 Apr 2021   Pseudo R-squ.:                  0.5699\n",
       "Time:                        15:17:39   Log-Likelihood:                -29.388\n",
       "converged:                       True   LL-Null:                       -68.331\n",
       "Covariance Type:            nonrobust   LLR p-value:                 4.243e-13\n",
       "================================================================================\n",
       "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "const          -14.9324      9.737     -1.534      0.125     -34.017       4.152\n",
       "GateDiam        -4.8123      2.709     -1.776      0.076     -10.123       0.498\n",
       "CupHeight       -0.5460      0.867     -0.630      0.529      -2.245       1.153\n",
       "SprueHeight     -0.2529      0.663     -0.382      0.703      -1.552       1.046\n",
       "RiserDiam        5.1473      1.155      4.457      0.000       2.884       7.411\n",
       "SprueDiamBot   -13.7618      4.284     -3.213      0.001     -22.158      -5.366\n",
       "SprueDiamTop    -8.8910      2.445     -3.637      0.000     -13.683      -4.099\n",
       "RiserHeight      0.5067      0.875      0.579      0.562      -1.208       2.221\n",
       "Riser1Pos       -0.0719      0.839     -0.086      0.932      -1.717       1.573\n",
       "Riser2Pos       -0.2144      0.766     -0.280      0.780      -1.716       1.287\n",
       "================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pull out our response variable\n",
    "y = df[\"Feasible\"]\n",
    "# df[predictors] is a dataframe that consists only of the predictors\n",
    "# sm.add_constant adds a column of 1's and allows sm.Logit to fit an intercept \n",
    "X = sm.add_constant(df[predictors])\n",
    "# fit the model and return the regression results\n",
    "model = sm.Logit(y, X).fit()\n",
    "results = model.summary()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i2RAP_2vu8Hu"
   },
   "source": [
    "The `Logit()` function gives the natural log of the odds that y (\"Feasible\") equals one of the categories (0 or 1). If we were to use the regular values of y, as opposed to $ln(P(Y = y))$, for the response/dependent variable and tried to fit a line, it wouldn’t be a very good representation of the relationship. If you don't believe us, give linear regression a try and see what you find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question -- what gives you this indication?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HK35d4GBxPcQ"
   },
   "source": [
    "## Question 2\n",
    "\n",
    "Interpret the summary output: Which variables appear to be statistically\n",
    "significant at the 95% level, and how do they affect feasibility? Are the results from the\n",
    "logistic regression consistent with your interpretation of the boxplots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUGembJUCy-7"
   },
   "source": [
    "Ans: Statistically Significant at 95%: RiserDiam, SpruceDiamBot, SpruceDiamTop. As RiserDiam increases, probability of feasibility increases. As SpruceDiamBot and SpruceDiamTop decreases, probability of feasibility decreases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_CzwLpFA7fk_"
   },
   "source": [
    "## Minimum AIC\n",
    "\n",
    "We are now going to find a model with the minimum AIC. We first split our data into training and test, use the training data to select variables to include in our model, and then estimate our model parameters on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JkUGTyf_zFa0"
   },
   "outputs": [],
   "source": [
    "# this function returns a logistic regression model with the minimum AIC\n",
    "def minAIC_Logit(X,y):\n",
    "    variables = X.columns\n",
    "    model = sm.Logit(y,X[variables]).fit()\n",
    "    while True:\n",
    "        print(f'old model aic: {model.aic}')\n",
    "        maxp = np.max(model.pvalues)\n",
    "        newvariables = variables[model.pvalues < maxp]\n",
    "        removed = variables[model.pvalues == maxp].values\n",
    "        print(f'considering a model with these variables removed: {removed}')\n",
    "        newmodel = sm.Logit(y,X[newvariables]).fit()\n",
    "        print(f'new model aic: {newmodel.aic}')\n",
    "        if newmodel.aic < model.aic:\n",
    "            model = newmodel\n",
    "            variables = newvariables\n",
    "        else:\n",
    "            break\n",
    "    return model,variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.266885\n",
      "         Iterations 9\n",
      "old model aic: 46.68845352078558\n",
      "considering a model with these variables removed: ['Riser1Pos']\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.267337\n",
      "         Iterations 9\n",
      "new model aic: 44.73373156451505\n",
      "old model aic: 44.73373156451505\n",
      "considering a model with these variables removed: ['GateDiam']\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.271203\n",
      "         Iterations 9\n",
      "new model aic: 43.1203432936762\n",
      "old model aic: 43.1203432936762\n",
      "considering a model with these variables removed: ['SprueHeight']\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.277104\n",
      "         Iterations 9\n",
      "new model aic: 41.710357577736325\n",
      "old model aic: 41.710357577736325\n",
      "considering a model with these variables removed: ['CupHeight']\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.289596\n",
      "         Iterations 8\n",
      "new model aic: 40.959608731778474\n",
      "old model aic: 40.959608731778474\n",
      "considering a model with these variables removed: ['Riser2Pos']\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.301619\n",
      "         Iterations 8\n",
      "new model aic: 40.16194603591265\n",
      "old model aic: 40.16194603591265\n",
      "considering a model with these variables removed: ['RiserHeight']\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.329331\n",
      "         Iterations 8\n",
      "new model aic: 40.93312784500093\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "\n",
    "# now call the minAIC function on our predictors and response variables\n",
    "new_train_model, logit_variables = minAIC_Logit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.282515\n",
      "         Iterations 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>Feasible</td>     <th>  No. Observations:  </th>  <td>    50</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>    45</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     4</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Tue, 06 Apr 2021</td> <th>  Pseudo R-squ.:     </th>  <td>0.5847</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>15:30:51</td>     <th>  Log-Likelihood:    </th> <td> -14.126</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -34.015</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>4.812e-08</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>        <td>  -10.8524</td> <td>    9.423</td> <td>   -1.152</td> <td> 0.249</td> <td>  -29.321</td> <td>    7.616</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RiserDiam</th>    <td>    5.1746</td> <td>    1.602</td> <td>    3.230</td> <td> 0.001</td> <td>    2.035</td> <td>    8.315</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SprueDiamBot</th> <td>  -15.4139</td> <td>    6.728</td> <td>   -2.291</td> <td> 0.022</td> <td>  -28.600</td> <td>   -2.228</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SprueDiamTop</th> <td>   -9.4564</td> <td>    3.295</td> <td>   -2.870</td> <td> 0.004</td> <td>  -15.914</td> <td>   -2.999</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RiserHeight</th>  <td>   -1.8547</td> <td>    1.520</td> <td>   -1.220</td> <td> 0.222</td> <td>   -4.833</td> <td>    1.124</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:               Feasible   No. Observations:                   50\n",
       "Model:                          Logit   Df Residuals:                       45\n",
       "Method:                           MLE   Df Model:                            4\n",
       "Date:                Tue, 06 Apr 2021   Pseudo R-squ.:                  0.5847\n",
       "Time:                        15:30:51   Log-Likelihood:                -14.126\n",
       "converged:                       True   LL-Null:                       -34.015\n",
       "Covariance Type:            nonrobust   LLR p-value:                 4.812e-08\n",
       "================================================================================\n",
       "                   coef    std err          z      P>|z|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "const          -10.8524      9.423     -1.152      0.249     -29.321       7.616\n",
       "RiserDiam        5.1746      1.602      3.230      0.001       2.035       8.315\n",
       "SprueDiamBot   -15.4139      6.728     -2.291      0.022     -28.600      -2.228\n",
       "SprueDiamTop    -9.4564      3.295     -2.870      0.004     -15.914      -2.999\n",
       "RiserHeight     -1.8547      1.520     -1.220      0.222      -4.833       1.124\n",
       "================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now fit the variables selected, using the test data\n",
    "new_model = sm.Logit(y_test, X_test[logit_variables]).fit()\n",
    "results = new_model.summary()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PTKa-LCV4FYH"
   },
   "source": [
    "## Question 3\n",
    "\n",
    "What has changed compared to the previous model? Have the variables been included that appeared to have the biggest impact on Feasible based on the box plots?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qa6ENHD9IUez"
   },
   "source": [
    "Ans: Only 4 variable from the initial model remain: RiserDiam, SpruceDiamBot, SpruceDiamTop (the three variables I indicated had the biggest impact on Feasible based on the box plots) and RiserHeight."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "Using the test data, what is the AIC of the original model with all of the predictors?\n",
    "Again using the test data, what is the AIC of the new model that we selected using minAIC_Logit? You may see that the AIC decreased.\n",
    "\n",
    "Hint: Look at minAIC_Logit to see how to compute the AIC from a trained logistic regression model\n",
    "\n",
    "You may see an error message saying the Maximum Likelihood optimization failed to converge. This is ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.000000\n",
      "         Iterations: 35\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.282515\n",
      "         Iterations 8\n",
      "20.00000020960844 38.25154774554039 40.16194603591265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bls24\\Anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:512: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  \"Check mle_retvals\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Your code here to answer Question 4\n",
    "oldmodel_aic = sm.Logit(y_test,X_test).fit().aic\n",
    "newmodel_aic = sm.Logit(y_test,X_test[logit_variables]).fit().aic\n",
    "print(oldmodel_aic, newmodel_aic, new_train_model.aic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "65He_uwhJ7S8"
   },
   "source": [
    "# Multivariate Regression\n",
    "\n",
    "We will now fit a multivariate regression model for batch time using the same predictors.\n",
    "\n",
    "Fit a multivariate regression model, using the code below. All mold-variables are used and additional non-linear terms in GateDiam are added. This model was suggested by the analysis in the previous lab.\n",
    "\n",
    "Feasible is not used since it is a response, not a mold-variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>GateDiam</th>\n",
       "      <th>CupHeight</th>\n",
       "      <th>SprueHeight</th>\n",
       "      <th>RiserDiam</th>\n",
       "      <th>SprueDiamBot</th>\n",
       "      <th>SprueDiamTop</th>\n",
       "      <th>RiserHeight</th>\n",
       "      <th>Riser1Pos</th>\n",
       "      <th>Riser2Pos</th>\n",
       "      <th>GateDiamSq</th>\n",
       "      <th>GateDiamCube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.318317</td>\n",
       "      <td>5.604768</td>\n",
       "      <td>10.148839</td>\n",
       "      <td>6.745247</td>\n",
       "      <td>0.635632</td>\n",
       "      <td>0.889822</td>\n",
       "      <td>5.318354</td>\n",
       "      <td>3.168774</td>\n",
       "      <td>5.233322</td>\n",
       "      <td>0.101326</td>\n",
       "      <td>0.032254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.353834</td>\n",
       "      <td>5.846289</td>\n",
       "      <td>9.528183</td>\n",
       "      <td>7.345273</td>\n",
       "      <td>0.642489</td>\n",
       "      <td>1.005683</td>\n",
       "      <td>4.836570</td>\n",
       "      <td>3.265961</td>\n",
       "      <td>5.482437</td>\n",
       "      <td>0.125198</td>\n",
       "      <td>0.044299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.596163</td>\n",
       "      <td>4.280124</td>\n",
       "      <td>10.887751</td>\n",
       "      <td>6.952856</td>\n",
       "      <td>0.317180</td>\n",
       "      <td>0.589558</td>\n",
       "      <td>5.492964</td>\n",
       "      <td>4.029533</td>\n",
       "      <td>4.719791</td>\n",
       "      <td>0.355411</td>\n",
       "      <td>0.211883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.484338</td>\n",
       "      <td>5.205026</td>\n",
       "      <td>11.003688</td>\n",
       "      <td>7.254450</td>\n",
       "      <td>0.531424</td>\n",
       "      <td>0.595769</td>\n",
       "      <td>4.897376</td>\n",
       "      <td>4.011947</td>\n",
       "      <td>4.822747</td>\n",
       "      <td>0.234583</td>\n",
       "      <td>0.113617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.325979</td>\n",
       "      <td>4.590546</td>\n",
       "      <td>9.953619</td>\n",
       "      <td>7.193640</td>\n",
       "      <td>0.335183</td>\n",
       "      <td>0.796535</td>\n",
       "      <td>5.026199</td>\n",
       "      <td>3.858020</td>\n",
       "      <td>4.766860</td>\n",
       "      <td>0.106262</td>\n",
       "      <td>0.034639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.385734</td>\n",
       "      <td>4.641756</td>\n",
       "      <td>10.612642</td>\n",
       "      <td>6.927531</td>\n",
       "      <td>0.616726</td>\n",
       "      <td>0.441190</td>\n",
       "      <td>4.257671</td>\n",
       "      <td>3.530392</td>\n",
       "      <td>5.122950</td>\n",
       "      <td>0.148791</td>\n",
       "      <td>0.057394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.545411</td>\n",
       "      <td>5.010564</td>\n",
       "      <td>9.585376</td>\n",
       "      <td>7.559404</td>\n",
       "      <td>0.256138</td>\n",
       "      <td>0.928311</td>\n",
       "      <td>4.947248</td>\n",
       "      <td>2.969497</td>\n",
       "      <td>4.923213</td>\n",
       "      <td>0.297473</td>\n",
       "      <td>0.162245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496098</td>\n",
       "      <td>5.127981</td>\n",
       "      <td>10.586198</td>\n",
       "      <td>7.677615</td>\n",
       "      <td>0.471807</td>\n",
       "      <td>0.876945</td>\n",
       "      <td>4.735738</td>\n",
       "      <td>3.662503</td>\n",
       "      <td>4.791633</td>\n",
       "      <td>0.246113</td>\n",
       "      <td>0.122096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.323735</td>\n",
       "      <td>4.763133</td>\n",
       "      <td>10.171436</td>\n",
       "      <td>7.624297</td>\n",
       "      <td>0.347353</td>\n",
       "      <td>0.615776</td>\n",
       "      <td>4.576749</td>\n",
       "      <td>3.885155</td>\n",
       "      <td>5.418784</td>\n",
       "      <td>0.104804</td>\n",
       "      <td>0.033929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.439625</td>\n",
       "      <td>5.000552</td>\n",
       "      <td>10.079735</td>\n",
       "      <td>6.809712</td>\n",
       "      <td>0.514110</td>\n",
       "      <td>1.073594</td>\n",
       "      <td>4.605566</td>\n",
       "      <td>3.048861</td>\n",
       "      <td>4.935211</td>\n",
       "      <td>0.193270</td>\n",
       "      <td>0.084966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.731471</td>\n",
       "      <td>5.566265</td>\n",
       "      <td>9.844369</td>\n",
       "      <td>6.608995</td>\n",
       "      <td>0.614395</td>\n",
       "      <td>0.896503</td>\n",
       "      <td>4.365152</td>\n",
       "      <td>3.578335</td>\n",
       "      <td>4.141080</td>\n",
       "      <td>0.535050</td>\n",
       "      <td>0.391374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.391366</td>\n",
       "      <td>5.089341</td>\n",
       "      <td>9.561708</td>\n",
       "      <td>6.142542</td>\n",
       "      <td>0.495450</td>\n",
       "      <td>0.833643</td>\n",
       "      <td>4.648696</td>\n",
       "      <td>3.731724</td>\n",
       "      <td>5.105492</td>\n",
       "      <td>0.153167</td>\n",
       "      <td>0.059944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.478188</td>\n",
       "      <td>5.371985</td>\n",
       "      <td>10.660073</td>\n",
       "      <td>7.085431</td>\n",
       "      <td>0.512858</td>\n",
       "      <td>0.730271</td>\n",
       "      <td>4.910340</td>\n",
       "      <td>3.398778</td>\n",
       "      <td>5.083974</td>\n",
       "      <td>0.228663</td>\n",
       "      <td>0.109344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.542769</td>\n",
       "      <td>5.376764</td>\n",
       "      <td>9.749828</td>\n",
       "      <td>6.869321</td>\n",
       "      <td>0.655590</td>\n",
       "      <td>0.624921</td>\n",
       "      <td>4.423302</td>\n",
       "      <td>3.114693</td>\n",
       "      <td>5.184459</td>\n",
       "      <td>0.294599</td>\n",
       "      <td>0.159899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.408595</td>\n",
       "      <td>4.564040</td>\n",
       "      <td>10.866439</td>\n",
       "      <td>8.249198</td>\n",
       "      <td>0.557073</td>\n",
       "      <td>0.321120</td>\n",
       "      <td>4.339523</td>\n",
       "      <td>3.432651</td>\n",
       "      <td>4.723476</td>\n",
       "      <td>0.166950</td>\n",
       "      <td>0.068215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.560750</td>\n",
       "      <td>4.891544</td>\n",
       "      <td>10.467268</td>\n",
       "      <td>6.704820</td>\n",
       "      <td>0.444561</td>\n",
       "      <td>0.625547</td>\n",
       "      <td>3.864137</td>\n",
       "      <td>2.999109</td>\n",
       "      <td>4.890355</td>\n",
       "      <td>0.314441</td>\n",
       "      <td>0.176323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.575437</td>\n",
       "      <td>5.150568</td>\n",
       "      <td>9.708410</td>\n",
       "      <td>6.783000</td>\n",
       "      <td>0.672232</td>\n",
       "      <td>0.909837</td>\n",
       "      <td>5.179206</td>\n",
       "      <td>3.573416</td>\n",
       "      <td>5.079847</td>\n",
       "      <td>0.331127</td>\n",
       "      <td>0.190543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.464480</td>\n",
       "      <td>5.567692</td>\n",
       "      <td>11.877841</td>\n",
       "      <td>7.060567</td>\n",
       "      <td>0.375930</td>\n",
       "      <td>0.876947</td>\n",
       "      <td>5.048341</td>\n",
       "      <td>3.696205</td>\n",
       "      <td>5.445477</td>\n",
       "      <td>0.215742</td>\n",
       "      <td>0.100208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.471123</td>\n",
       "      <td>4.660616</td>\n",
       "      <td>10.383892</td>\n",
       "      <td>7.120236</td>\n",
       "      <td>0.454782</td>\n",
       "      <td>1.107137</td>\n",
       "      <td>5.396515</td>\n",
       "      <td>3.644455</td>\n",
       "      <td>5.109645</td>\n",
       "      <td>0.221957</td>\n",
       "      <td>0.104569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.290067</td>\n",
       "      <td>4.808493</td>\n",
       "      <td>10.587120</td>\n",
       "      <td>7.151923</td>\n",
       "      <td>0.439785</td>\n",
       "      <td>0.772407</td>\n",
       "      <td>5.322923</td>\n",
       "      <td>3.135791</td>\n",
       "      <td>4.934025</td>\n",
       "      <td>0.084139</td>\n",
       "      <td>0.024406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.606628</td>\n",
       "      <td>4.686982</td>\n",
       "      <td>10.432046</td>\n",
       "      <td>7.505320</td>\n",
       "      <td>0.546549</td>\n",
       "      <td>0.716170</td>\n",
       "      <td>5.366995</td>\n",
       "      <td>3.289785</td>\n",
       "      <td>3.629352</td>\n",
       "      <td>0.367998</td>\n",
       "      <td>0.223238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.370381</td>\n",
       "      <td>5.714639</td>\n",
       "      <td>10.591229</td>\n",
       "      <td>6.349178</td>\n",
       "      <td>0.399187</td>\n",
       "      <td>0.656558</td>\n",
       "      <td>5.587719</td>\n",
       "      <td>3.193236</td>\n",
       "      <td>5.349328</td>\n",
       "      <td>0.137182</td>\n",
       "      <td>0.050809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.500835</td>\n",
       "      <td>5.457467</td>\n",
       "      <td>10.874218</td>\n",
       "      <td>7.258645</td>\n",
       "      <td>0.385900</td>\n",
       "      <td>0.759439</td>\n",
       "      <td>4.960431</td>\n",
       "      <td>3.064471</td>\n",
       "      <td>4.948846</td>\n",
       "      <td>0.250836</td>\n",
       "      <td>0.125627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.324456</td>\n",
       "      <td>5.022684</td>\n",
       "      <td>9.774023</td>\n",
       "      <td>5.980219</td>\n",
       "      <td>0.355030</td>\n",
       "      <td>0.739161</td>\n",
       "      <td>5.032147</td>\n",
       "      <td>3.685030</td>\n",
       "      <td>4.335273</td>\n",
       "      <td>0.105272</td>\n",
       "      <td>0.034156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.600706</td>\n",
       "      <td>5.431578</td>\n",
       "      <td>10.037868</td>\n",
       "      <td>6.470382</td>\n",
       "      <td>0.410487</td>\n",
       "      <td>0.959120</td>\n",
       "      <td>5.434723</td>\n",
       "      <td>2.970682</td>\n",
       "      <td>5.952806</td>\n",
       "      <td>0.360848</td>\n",
       "      <td>0.216763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.401358</td>\n",
       "      <td>4.616213</td>\n",
       "      <td>9.503569</td>\n",
       "      <td>8.147551</td>\n",
       "      <td>0.451821</td>\n",
       "      <td>0.659277</td>\n",
       "      <td>5.046371</td>\n",
       "      <td>3.592176</td>\n",
       "      <td>4.685772</td>\n",
       "      <td>0.161088</td>\n",
       "      <td>0.064654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.880842</td>\n",
       "      <td>4.926109</td>\n",
       "      <td>9.654262</td>\n",
       "      <td>7.884367</td>\n",
       "      <td>0.452449</td>\n",
       "      <td>0.867692</td>\n",
       "      <td>4.958081</td>\n",
       "      <td>3.552331</td>\n",
       "      <td>5.599510</td>\n",
       "      <td>0.775883</td>\n",
       "      <td>0.683430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540809</td>\n",
       "      <td>5.403479</td>\n",
       "      <td>9.702274</td>\n",
       "      <td>6.621438</td>\n",
       "      <td>0.507075</td>\n",
       "      <td>0.808285</td>\n",
       "      <td>5.978733</td>\n",
       "      <td>3.682722</td>\n",
       "      <td>5.174093</td>\n",
       "      <td>0.292475</td>\n",
       "      <td>0.158173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.270931</td>\n",
       "      <td>4.627201</td>\n",
       "      <td>10.043514</td>\n",
       "      <td>6.194518</td>\n",
       "      <td>0.511781</td>\n",
       "      <td>0.847970</td>\n",
       "      <td>4.722791</td>\n",
       "      <td>3.108218</td>\n",
       "      <td>4.458796</td>\n",
       "      <td>0.073403</td>\n",
       "      <td>0.019887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.563880</td>\n",
       "      <td>4.438597</td>\n",
       "      <td>9.595602</td>\n",
       "      <td>6.599158</td>\n",
       "      <td>0.591484</td>\n",
       "      <td>0.440910</td>\n",
       "      <td>4.160772</td>\n",
       "      <td>3.242366</td>\n",
       "      <td>5.328850</td>\n",
       "      <td>0.317961</td>\n",
       "      <td>0.179292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.488949</td>\n",
       "      <td>5.499100</td>\n",
       "      <td>10.560839</td>\n",
       "      <td>7.381354</td>\n",
       "      <td>0.664812</td>\n",
       "      <td>0.860837</td>\n",
       "      <td>5.530691</td>\n",
       "      <td>3.844025</td>\n",
       "      <td>5.651443</td>\n",
       "      <td>0.239071</td>\n",
       "      <td>0.116894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.382239</td>\n",
       "      <td>4.216774</td>\n",
       "      <td>9.987825</td>\n",
       "      <td>6.711839</td>\n",
       "      <td>0.847226</td>\n",
       "      <td>0.506477</td>\n",
       "      <td>4.574079</td>\n",
       "      <td>3.559388</td>\n",
       "      <td>5.957579</td>\n",
       "      <td>0.146107</td>\n",
       "      <td>0.055848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.370965</td>\n",
       "      <td>5.122562</td>\n",
       "      <td>9.412002</td>\n",
       "      <td>6.683092</td>\n",
       "      <td>0.405863</td>\n",
       "      <td>0.560822</td>\n",
       "      <td>5.398677</td>\n",
       "      <td>3.314134</td>\n",
       "      <td>5.132011</td>\n",
       "      <td>0.137615</td>\n",
       "      <td>0.051050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.482849</td>\n",
       "      <td>5.260444</td>\n",
       "      <td>10.477875</td>\n",
       "      <td>8.303038</td>\n",
       "      <td>0.486564</td>\n",
       "      <td>0.390664</td>\n",
       "      <td>5.205090</td>\n",
       "      <td>4.009494</td>\n",
       "      <td>4.662281</td>\n",
       "      <td>0.233143</td>\n",
       "      <td>0.112573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.664248</td>\n",
       "      <td>5.375943</td>\n",
       "      <td>9.657140</td>\n",
       "      <td>6.757692</td>\n",
       "      <td>0.448336</td>\n",
       "      <td>0.790383</td>\n",
       "      <td>5.205651</td>\n",
       "      <td>4.348254</td>\n",
       "      <td>4.851813</td>\n",
       "      <td>0.441225</td>\n",
       "      <td>0.293083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.214947</td>\n",
       "      <td>4.980209</td>\n",
       "      <td>9.799755</td>\n",
       "      <td>7.172251</td>\n",
       "      <td>0.727846</td>\n",
       "      <td>0.412729</td>\n",
       "      <td>5.415931</td>\n",
       "      <td>3.082556</td>\n",
       "      <td>4.837880</td>\n",
       "      <td>0.046202</td>\n",
       "      <td>0.009931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.605046</td>\n",
       "      <td>4.451586</td>\n",
       "      <td>11.113131</td>\n",
       "      <td>6.558839</td>\n",
       "      <td>0.389330</td>\n",
       "      <td>0.905005</td>\n",
       "      <td>5.545835</td>\n",
       "      <td>3.253251</td>\n",
       "      <td>5.103267</td>\n",
       "      <td>0.366080</td>\n",
       "      <td>0.221495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.457540</td>\n",
       "      <td>5.411050</td>\n",
       "      <td>9.297743</td>\n",
       "      <td>7.046685</td>\n",
       "      <td>0.715304</td>\n",
       "      <td>0.569631</td>\n",
       "      <td>4.944231</td>\n",
       "      <td>3.046658</td>\n",
       "      <td>4.804468</td>\n",
       "      <td>0.209343</td>\n",
       "      <td>0.095783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.713104</td>\n",
       "      <td>5.368698</td>\n",
       "      <td>9.862707</td>\n",
       "      <td>7.494085</td>\n",
       "      <td>0.452853</td>\n",
       "      <td>0.670708</td>\n",
       "      <td>5.252929</td>\n",
       "      <td>3.831724</td>\n",
       "      <td>4.700450</td>\n",
       "      <td>0.508517</td>\n",
       "      <td>0.362626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.383176</td>\n",
       "      <td>4.943641</td>\n",
       "      <td>9.595162</td>\n",
       "      <td>5.472633</td>\n",
       "      <td>0.443152</td>\n",
       "      <td>0.591842</td>\n",
       "      <td>4.347219</td>\n",
       "      <td>3.774483</td>\n",
       "      <td>5.188029</td>\n",
       "      <td>0.146824</td>\n",
       "      <td>0.056259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.419387</td>\n",
       "      <td>5.162315</td>\n",
       "      <td>9.289095</td>\n",
       "      <td>7.514892</td>\n",
       "      <td>0.360014</td>\n",
       "      <td>0.798711</td>\n",
       "      <td>5.300113</td>\n",
       "      <td>3.224923</td>\n",
       "      <td>3.965430</td>\n",
       "      <td>0.175886</td>\n",
       "      <td>0.073764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.883781</td>\n",
       "      <td>5.047806</td>\n",
       "      <td>10.409854</td>\n",
       "      <td>7.233535</td>\n",
       "      <td>0.445911</td>\n",
       "      <td>1.051397</td>\n",
       "      <td>5.503633</td>\n",
       "      <td>3.351190</td>\n",
       "      <td>5.109417</td>\n",
       "      <td>0.781070</td>\n",
       "      <td>0.690295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.463731</td>\n",
       "      <td>5.361486</td>\n",
       "      <td>9.298958</td>\n",
       "      <td>7.140436</td>\n",
       "      <td>0.488428</td>\n",
       "      <td>0.739002</td>\n",
       "      <td>5.419529</td>\n",
       "      <td>3.007564</td>\n",
       "      <td>4.408599</td>\n",
       "      <td>0.215046</td>\n",
       "      <td>0.099723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447679</td>\n",
       "      <td>5.096040</td>\n",
       "      <td>10.316412</td>\n",
       "      <td>6.228459</td>\n",
       "      <td>0.445073</td>\n",
       "      <td>0.707734</td>\n",
       "      <td>5.256341</td>\n",
       "      <td>2.808385</td>\n",
       "      <td>4.521664</td>\n",
       "      <td>0.200417</td>\n",
       "      <td>0.089722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.286752</td>\n",
       "      <td>4.822884</td>\n",
       "      <td>10.218420</td>\n",
       "      <td>6.839080</td>\n",
       "      <td>0.433420</td>\n",
       "      <td>0.509731</td>\n",
       "      <td>5.281467</td>\n",
       "      <td>3.330799</td>\n",
       "      <td>5.033389</td>\n",
       "      <td>0.082227</td>\n",
       "      <td>0.023579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.448719</td>\n",
       "      <td>5.025592</td>\n",
       "      <td>10.540438</td>\n",
       "      <td>6.646488</td>\n",
       "      <td>0.507571</td>\n",
       "      <td>0.690082</td>\n",
       "      <td>5.764571</td>\n",
       "      <td>3.927160</td>\n",
       "      <td>5.513324</td>\n",
       "      <td>0.201349</td>\n",
       "      <td>0.090349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.539343</td>\n",
       "      <td>5.372104</td>\n",
       "      <td>10.141012</td>\n",
       "      <td>6.271291</td>\n",
       "      <td>0.497659</td>\n",
       "      <td>0.932917</td>\n",
       "      <td>4.979550</td>\n",
       "      <td>3.570394</td>\n",
       "      <td>5.005341</td>\n",
       "      <td>0.290891</td>\n",
       "      <td>0.156890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.382416</td>\n",
       "      <td>5.023000</td>\n",
       "      <td>9.267266</td>\n",
       "      <td>6.762023</td>\n",
       "      <td>0.419572</td>\n",
       "      <td>1.231756</td>\n",
       "      <td>5.005763</td>\n",
       "      <td>3.628582</td>\n",
       "      <td>4.423851</td>\n",
       "      <td>0.146242</td>\n",
       "      <td>0.055925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.514084</td>\n",
       "      <td>4.706547</td>\n",
       "      <td>11.072051</td>\n",
       "      <td>7.892895</td>\n",
       "      <td>0.611000</td>\n",
       "      <td>0.668141</td>\n",
       "      <td>5.255425</td>\n",
       "      <td>3.679835</td>\n",
       "      <td>5.330818</td>\n",
       "      <td>0.264282</td>\n",
       "      <td>0.135863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.236190</td>\n",
       "      <td>4.716842</td>\n",
       "      <td>9.508842</td>\n",
       "      <td>6.214093</td>\n",
       "      <td>0.437528</td>\n",
       "      <td>0.804666</td>\n",
       "      <td>4.079077</td>\n",
       "      <td>2.949836</td>\n",
       "      <td>5.464137</td>\n",
       "      <td>0.055786</td>\n",
       "      <td>0.013176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.607835</td>\n",
       "      <td>4.793052</td>\n",
       "      <td>9.729212</td>\n",
       "      <td>6.244389</td>\n",
       "      <td>0.849809</td>\n",
       "      <td>0.756392</td>\n",
       "      <td>4.742408</td>\n",
       "      <td>3.271565</td>\n",
       "      <td>4.808678</td>\n",
       "      <td>0.369463</td>\n",
       "      <td>0.224573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.413854</td>\n",
       "      <td>5.390766</td>\n",
       "      <td>9.527699</td>\n",
       "      <td>6.959865</td>\n",
       "      <td>0.630166</td>\n",
       "      <td>0.708142</td>\n",
       "      <td>4.777028</td>\n",
       "      <td>4.387350</td>\n",
       "      <td>4.664976</td>\n",
       "      <td>0.171275</td>\n",
       "      <td>0.070883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.279804</td>\n",
       "      <td>5.453046</td>\n",
       "      <td>10.644390</td>\n",
       "      <td>6.968638</td>\n",
       "      <td>0.547557</td>\n",
       "      <td>0.927792</td>\n",
       "      <td>4.968167</td>\n",
       "      <td>3.130822</td>\n",
       "      <td>4.707302</td>\n",
       "      <td>0.078290</td>\n",
       "      <td>0.021906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.509607</td>\n",
       "      <td>5.204070</td>\n",
       "      <td>9.840143</td>\n",
       "      <td>7.554053</td>\n",
       "      <td>0.410427</td>\n",
       "      <td>0.758669</td>\n",
       "      <td>4.753214</td>\n",
       "      <td>3.725259</td>\n",
       "      <td>4.897358</td>\n",
       "      <td>0.259700</td>\n",
       "      <td>0.132345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.535138</td>\n",
       "      <td>4.731912</td>\n",
       "      <td>9.851363</td>\n",
       "      <td>6.754550</td>\n",
       "      <td>0.476329</td>\n",
       "      <td>1.078394</td>\n",
       "      <td>4.773315</td>\n",
       "      <td>3.585070</td>\n",
       "      <td>5.561820</td>\n",
       "      <td>0.286373</td>\n",
       "      <td>0.153249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.255820</td>\n",
       "      <td>5.482461</td>\n",
       "      <td>11.287504</td>\n",
       "      <td>6.179215</td>\n",
       "      <td>0.590127</td>\n",
       "      <td>0.726530</td>\n",
       "      <td>5.412255</td>\n",
       "      <td>3.425544</td>\n",
       "      <td>4.578372</td>\n",
       "      <td>0.065444</td>\n",
       "      <td>0.016742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.745953</td>\n",
       "      <td>5.008650</td>\n",
       "      <td>9.989357</td>\n",
       "      <td>7.412295</td>\n",
       "      <td>0.624900</td>\n",
       "      <td>0.641878</td>\n",
       "      <td>4.632735</td>\n",
       "      <td>3.977273</td>\n",
       "      <td>5.305690</td>\n",
       "      <td>0.556445</td>\n",
       "      <td>0.415082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.443939</td>\n",
       "      <td>4.782634</td>\n",
       "      <td>10.488185</td>\n",
       "      <td>7.285994</td>\n",
       "      <td>0.528419</td>\n",
       "      <td>0.883926</td>\n",
       "      <td>4.503812</td>\n",
       "      <td>3.572636</td>\n",
       "      <td>5.061096</td>\n",
       "      <td>0.197082</td>\n",
       "      <td>0.087493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.633789</td>\n",
       "      <td>4.831297</td>\n",
       "      <td>8.848969</td>\n",
       "      <td>7.668250</td>\n",
       "      <td>0.510992</td>\n",
       "      <td>0.999930</td>\n",
       "      <td>4.775548</td>\n",
       "      <td>3.410252</td>\n",
       "      <td>5.492044</td>\n",
       "      <td>0.401689</td>\n",
       "      <td>0.254586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    const  GateDiam  CupHeight  SprueHeight  RiserDiam  SprueDiamBot  \\\n",
       "0     1.0  0.500000   5.000000    10.000000   7.000000      0.500000   \n",
       "1     1.0  0.318317   5.604768    10.148839   6.745247      0.635632   \n",
       "2     1.0  0.353834   5.846289     9.528183   7.345273      0.642489   \n",
       "3     1.0  0.596163   4.280124    10.887751   6.952856      0.317180   \n",
       "4     1.0  0.484338   5.205026    11.003688   7.254450      0.531424   \n",
       "5     1.0  0.325979   4.590546     9.953619   7.193640      0.335183   \n",
       "6     1.0  0.385734   4.641756    10.612642   6.927531      0.616726   \n",
       "7     1.0  0.545411   5.010564     9.585376   7.559404      0.256138   \n",
       "8     1.0  0.496098   5.127981    10.586198   7.677615      0.471807   \n",
       "9     1.0  0.323735   4.763133    10.171436   7.624297      0.347353   \n",
       "10    1.0  0.439625   5.000552    10.079735   6.809712      0.514110   \n",
       "11    1.0  0.731471   5.566265     9.844369   6.608995      0.614395   \n",
       "12    1.0  0.391366   5.089341     9.561708   6.142542      0.495450   \n",
       "13    1.0  0.478188   5.371985    10.660073   7.085431      0.512858   \n",
       "14    1.0  0.542769   5.376764     9.749828   6.869321      0.655590   \n",
       "15    1.0  0.408595   4.564040    10.866439   8.249198      0.557073   \n",
       "16    1.0  0.560750   4.891544    10.467268   6.704820      0.444561   \n",
       "17    1.0  0.575437   5.150568     9.708410   6.783000      0.672232   \n",
       "18    1.0  0.464480   5.567692    11.877841   7.060567      0.375930   \n",
       "19    1.0  0.471123   4.660616    10.383892   7.120236      0.454782   \n",
       "20    1.0  0.290067   4.808493    10.587120   7.151923      0.439785   \n",
       "21    1.0  0.606628   4.686982    10.432046   7.505320      0.546549   \n",
       "22    1.0  0.370381   5.714639    10.591229   6.349178      0.399187   \n",
       "23    1.0  0.500835   5.457467    10.874218   7.258645      0.385900   \n",
       "24    1.0  0.324456   5.022684     9.774023   5.980219      0.355030   \n",
       "25    1.0  0.600706   5.431578    10.037868   6.470382      0.410487   \n",
       "26    1.0  0.401358   4.616213     9.503569   8.147551      0.451821   \n",
       "27    1.0  0.880842   4.926109     9.654262   7.884367      0.452449   \n",
       "28    1.0  0.540809   5.403479     9.702274   6.621438      0.507075   \n",
       "29    1.0  0.270931   4.627201    10.043514   6.194518      0.511781   \n",
       "..    ...       ...        ...          ...        ...           ...   \n",
       "70    1.0  0.563880   4.438597     9.595602   6.599158      0.591484   \n",
       "71    1.0  0.488949   5.499100    10.560839   7.381354      0.664812   \n",
       "72    1.0  0.382239   4.216774     9.987825   6.711839      0.847226   \n",
       "73    1.0  0.370965   5.122562     9.412002   6.683092      0.405863   \n",
       "74    1.0  0.482849   5.260444    10.477875   8.303038      0.486564   \n",
       "75    1.0  0.664248   5.375943     9.657140   6.757692      0.448336   \n",
       "76    1.0  0.214947   4.980209     9.799755   7.172251      0.727846   \n",
       "77    1.0  0.605046   4.451586    11.113131   6.558839      0.389330   \n",
       "78    1.0  0.457540   5.411050     9.297743   7.046685      0.715304   \n",
       "79    1.0  0.713104   5.368698     9.862707   7.494085      0.452853   \n",
       "80    1.0  0.383176   4.943641     9.595162   5.472633      0.443152   \n",
       "81    1.0  0.419387   5.162315     9.289095   7.514892      0.360014   \n",
       "82    1.0  0.883781   5.047806    10.409854   7.233535      0.445911   \n",
       "83    1.0  0.463731   5.361486     9.298958   7.140436      0.488428   \n",
       "84    1.0  0.447679   5.096040    10.316412   6.228459      0.445073   \n",
       "85    1.0  0.286752   4.822884    10.218420   6.839080      0.433420   \n",
       "86    1.0  0.448719   5.025592    10.540438   6.646488      0.507571   \n",
       "87    1.0  0.539343   5.372104    10.141012   6.271291      0.497659   \n",
       "88    1.0  0.382416   5.023000     9.267266   6.762023      0.419572   \n",
       "89    1.0  0.514084   4.706547    11.072051   7.892895      0.611000   \n",
       "90    1.0  0.236190   4.716842     9.508842   6.214093      0.437528   \n",
       "91    1.0  0.607835   4.793052     9.729212   6.244389      0.849809   \n",
       "92    1.0  0.413854   5.390766     9.527699   6.959865      0.630166   \n",
       "93    1.0  0.279804   5.453046    10.644390   6.968638      0.547557   \n",
       "94    1.0  0.509607   5.204070     9.840143   7.554053      0.410427   \n",
       "95    1.0  0.535138   4.731912     9.851363   6.754550      0.476329   \n",
       "96    1.0  0.255820   5.482461    11.287504   6.179215      0.590127   \n",
       "97    1.0  0.745953   5.008650     9.989357   7.412295      0.624900   \n",
       "98    1.0  0.443939   4.782634    10.488185   7.285994      0.528419   \n",
       "99    1.0  0.633789   4.831297     8.848969   7.668250      0.510992   \n",
       "\n",
       "    SprueDiamTop  RiserHeight  Riser1Pos  Riser2Pos  GateDiamSq  GateDiamCube  \n",
       "0       0.750000     5.000000   3.500000   5.000000    0.250000      0.125000  \n",
       "1       0.889822     5.318354   3.168774   5.233322    0.101326      0.032254  \n",
       "2       1.005683     4.836570   3.265961   5.482437    0.125198      0.044299  \n",
       "3       0.589558     5.492964   4.029533   4.719791    0.355411      0.211883  \n",
       "4       0.595769     4.897376   4.011947   4.822747    0.234583      0.113617  \n",
       "5       0.796535     5.026199   3.858020   4.766860    0.106262      0.034639  \n",
       "6       0.441190     4.257671   3.530392   5.122950    0.148791      0.057394  \n",
       "7       0.928311     4.947248   2.969497   4.923213    0.297473      0.162245  \n",
       "8       0.876945     4.735738   3.662503   4.791633    0.246113      0.122096  \n",
       "9       0.615776     4.576749   3.885155   5.418784    0.104804      0.033929  \n",
       "10      1.073594     4.605566   3.048861   4.935211    0.193270      0.084966  \n",
       "11      0.896503     4.365152   3.578335   4.141080    0.535050      0.391374  \n",
       "12      0.833643     4.648696   3.731724   5.105492    0.153167      0.059944  \n",
       "13      0.730271     4.910340   3.398778   5.083974    0.228663      0.109344  \n",
       "14      0.624921     4.423302   3.114693   5.184459    0.294599      0.159899  \n",
       "15      0.321120     4.339523   3.432651   4.723476    0.166950      0.068215  \n",
       "16      0.625547     3.864137   2.999109   4.890355    0.314441      0.176323  \n",
       "17      0.909837     5.179206   3.573416   5.079847    0.331127      0.190543  \n",
       "18      0.876947     5.048341   3.696205   5.445477    0.215742      0.100208  \n",
       "19      1.107137     5.396515   3.644455   5.109645    0.221957      0.104569  \n",
       "20      0.772407     5.322923   3.135791   4.934025    0.084139      0.024406  \n",
       "21      0.716170     5.366995   3.289785   3.629352    0.367998      0.223238  \n",
       "22      0.656558     5.587719   3.193236   5.349328    0.137182      0.050809  \n",
       "23      0.759439     4.960431   3.064471   4.948846    0.250836      0.125627  \n",
       "24      0.739161     5.032147   3.685030   4.335273    0.105272      0.034156  \n",
       "25      0.959120     5.434723   2.970682   5.952806    0.360848      0.216763  \n",
       "26      0.659277     5.046371   3.592176   4.685772    0.161088      0.064654  \n",
       "27      0.867692     4.958081   3.552331   5.599510    0.775883      0.683430  \n",
       "28      0.808285     5.978733   3.682722   5.174093    0.292475      0.158173  \n",
       "29      0.847970     4.722791   3.108218   4.458796    0.073403      0.019887  \n",
       "..           ...          ...        ...        ...         ...           ...  \n",
       "70      0.440910     4.160772   3.242366   5.328850    0.317961      0.179292  \n",
       "71      0.860837     5.530691   3.844025   5.651443    0.239071      0.116894  \n",
       "72      0.506477     4.574079   3.559388   5.957579    0.146107      0.055848  \n",
       "73      0.560822     5.398677   3.314134   5.132011    0.137615      0.051050  \n",
       "74      0.390664     5.205090   4.009494   4.662281    0.233143      0.112573  \n",
       "75      0.790383     5.205651   4.348254   4.851813    0.441225      0.293083  \n",
       "76      0.412729     5.415931   3.082556   4.837880    0.046202      0.009931  \n",
       "77      0.905005     5.545835   3.253251   5.103267    0.366080      0.221495  \n",
       "78      0.569631     4.944231   3.046658   4.804468    0.209343      0.095783  \n",
       "79      0.670708     5.252929   3.831724   4.700450    0.508517      0.362626  \n",
       "80      0.591842     4.347219   3.774483   5.188029    0.146824      0.056259  \n",
       "81      0.798711     5.300113   3.224923   3.965430    0.175886      0.073764  \n",
       "82      1.051397     5.503633   3.351190   5.109417    0.781070      0.690295  \n",
       "83      0.739002     5.419529   3.007564   4.408599    0.215046      0.099723  \n",
       "84      0.707734     5.256341   2.808385   4.521664    0.200417      0.089722  \n",
       "85      0.509731     5.281467   3.330799   5.033389    0.082227      0.023579  \n",
       "86      0.690082     5.764571   3.927160   5.513324    0.201349      0.090349  \n",
       "87      0.932917     4.979550   3.570394   5.005341    0.290891      0.156890  \n",
       "88      1.231756     5.005763   3.628582   4.423851    0.146242      0.055925  \n",
       "89      0.668141     5.255425   3.679835   5.330818    0.264282      0.135863  \n",
       "90      0.804666     4.079077   2.949836   5.464137    0.055786      0.013176  \n",
       "91      0.756392     4.742408   3.271565   4.808678    0.369463      0.224573  \n",
       "92      0.708142     4.777028   4.387350   4.664976    0.171275      0.070883  \n",
       "93      0.927792     4.968167   3.130822   4.707302    0.078290      0.021906  \n",
       "94      0.758669     4.753214   3.725259   4.897358    0.259700      0.132345  \n",
       "95      1.078394     4.773315   3.585070   5.561820    0.286373      0.153249  \n",
       "96      0.726530     5.412255   3.425544   4.578372    0.065444      0.016742  \n",
       "97      0.641878     4.632735   3.977273   5.305690    0.556445      0.415082  \n",
       "98      0.883926     4.503812   3.572636   5.061096    0.197082      0.087493  \n",
       "99      0.999930     4.775548   3.410252   5.492044    0.401689      0.254586  \n",
       "\n",
       "[100 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['BatchTime']\n",
    "X = df[predictors].copy()\n",
    "X = sm.add_constant(X)\n",
    "X['GateDiamSq'] = X['GateDiam']**2\n",
    "X['GateDiamCube'] = X['GateDiam']**3\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>BatchTime</td>    <th>  R-squared:         </th> <td>   0.835</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.815</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   40.52</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 06 Apr 2021</td> <th>  Prob (F-statistic):</th> <td>1.01e-29</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:57:41</td>     <th>  Log-Likelihood:    </th> <td> -309.10</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   642.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    88</td>      <th>  BIC:               </th> <td>   673.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>    11</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>        <td> 3028.3393</td> <td>   20.371</td> <td>  148.662</td> <td> 0.000</td> <td> 2987.857</td> <td> 3068.822</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GateDiam</th>     <td> -476.1945</td> <td>   90.302</td> <td>   -5.273</td> <td> 0.000</td> <td> -655.651</td> <td> -296.738</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>CupHeight</th>    <td>    1.1435</td> <td>    1.519</td> <td>    0.753</td> <td> 0.454</td> <td>   -1.876</td> <td>    4.162</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SprueHeight</th>  <td>   -0.8335</td> <td>    1.056</td> <td>   -0.790</td> <td> 0.432</td> <td>   -2.931</td> <td>    1.264</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RiserDiam</th>    <td>    9.5573</td> <td>    1.141</td> <td>    8.374</td> <td> 0.000</td> <td>    7.289</td> <td>   11.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SprueDiamBot</th> <td>    4.5050</td> <td>    4.838</td> <td>    0.931</td> <td> 0.354</td> <td>   -5.110</td> <td>   14.120</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>SprueDiamTop</th> <td>    1.4360</td> <td>    3.480</td> <td>    0.413</td> <td> 0.681</td> <td>   -5.480</td> <td>    8.352</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RiserHeight</th>  <td>    9.1068</td> <td>    1.381</td> <td>    6.594</td> <td> 0.000</td> <td>    6.362</td> <td>   11.852</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Riser1Pos</th>    <td>   -0.7958</td> <td>    1.560</td> <td>   -0.510</td> <td> 0.611</td> <td>   -3.895</td> <td>    2.304</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Riser2Pos</th>    <td>    0.7674</td> <td>    1.263</td> <td>    0.608</td> <td> 0.545</td> <td>   -1.742</td> <td>    3.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GateDiamSq</th>   <td>  694.4074</td> <td>  176.257</td> <td>    3.940</td> <td> 0.000</td> <td>  344.134</td> <td> 1044.681</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GateDiamCube</th> <td> -367.9510</td> <td>  108.946</td> <td>   -3.377</td> <td> 0.001</td> <td> -584.457</td> <td> -151.445</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.361</td> <th>  Durbin-Watson:     </th> <td>   1.985</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.835</td> <th>  Jarque-Bera (JB):  </th> <td>   0.523</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.102</td> <th>  Prob(JB):          </th> <td>   0.770</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.710</td> <th>  Cond. No.          </th> <td>6.14e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 6.14e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              BatchTime   R-squared:                       0.835\n",
       "Model:                            OLS   Adj. R-squared:                  0.815\n",
       "Method:                 Least Squares   F-statistic:                     40.52\n",
       "Date:                Tue, 06 Apr 2021   Prob (F-statistic):           1.01e-29\n",
       "Time:                        15:57:41   Log-Likelihood:                -309.10\n",
       "No. Observations:                 100   AIC:                             642.2\n",
       "Df Residuals:                      88   BIC:                             673.5\n",
       "Df Model:                          11                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "const         3028.3393     20.371    148.662      0.000    2987.857    3068.822\n",
       "GateDiam      -476.1945     90.302     -5.273      0.000    -655.651    -296.738\n",
       "CupHeight        1.1435      1.519      0.753      0.454      -1.876       4.162\n",
       "SprueHeight     -0.8335      1.056     -0.790      0.432      -2.931       1.264\n",
       "RiserDiam        9.5573      1.141      8.374      0.000       7.289      11.825\n",
       "SprueDiamBot     4.5050      4.838      0.931      0.354      -5.110      14.120\n",
       "SprueDiamTop     1.4360      3.480      0.413      0.681      -5.480       8.352\n",
       "RiserHeight      9.1068      1.381      6.594      0.000       6.362      11.852\n",
       "Riser1Pos       -0.7958      1.560     -0.510      0.611      -3.895       2.304\n",
       "Riser2Pos        0.7674      1.263      0.608      0.545      -1.742       3.277\n",
       "GateDiamSq     694.4074    176.257      3.940      0.000     344.134    1044.681\n",
       "GateDiamCube  -367.9510    108.946     -3.377      0.001    -584.457    -151.445\n",
       "==============================================================================\n",
       "Omnibus:                        0.361   Durbin-Watson:                   1.985\n",
       "Prob(Omnibus):                  0.835   Jarque-Bera (JB):                0.523\n",
       "Skew:                          -0.102   Prob(JB):                        0.770\n",
       "Kurtosis:                       2.710   Cond. No.                     6.14e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 6.14e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_linear_model = sm.OLS(y,X).fit()\n",
    "new_linear_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z4SO7jbyehNM"
   },
   "source": [
    "Run the code below to the linear model and set of variables with minimal AIC. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T4KMwSIFgEM6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old model aic: 329.66481593469825\n",
      "considering a model with these variables removed: ['Riser1Pos']\n",
      "new model aic: 327.74841419183747\n",
      "old model aic: 327.74841419183747\n",
      "considering a model with these variables removed: ['SprueHeight']\n",
      "new model aic: 325.9815575360428\n",
      "old model aic: 325.9815575360428\n",
      "considering a model with these variables removed: ['Riser2Pos']\n",
      "new model aic: 324.264393371404\n",
      "old model aic: 324.264393371404\n",
      "considering a model with these variables removed: ['CupHeight']\n",
      "new model aic: 322.81051481441517\n",
      "old model aic: 322.81051481441517\n",
      "considering a model with these variables removed: ['SprueDiamTop']\n",
      "new model aic: 321.26476425047593\n",
      "old model aic: 321.26476425047593\n",
      "considering a model with these variables removed: ['SprueDiamBot']\n",
      "new model aic: 319.8010046012662\n",
      "old model aic: 319.8010046012662\n",
      "considering a model with these variables removed: ['GateDiamCube']\n",
      "new model aic: 322.94644533458006\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>BatchTime</td>    <th>  R-squared:         </th> <td>   0.825</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.805</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   41.55</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Tue, 06 Apr 2021</td> <th>  Prob (F-statistic):</th> <td>1.38e-15</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>15:57:53</td>     <th>  Log-Likelihood:    </th> <td> -154.50</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    50</td>      <th>  AIC:               </th> <td>   321.0</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    44</td>      <th>  BIC:               </th> <td>   332.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "        <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>        <td> 3028.1264</td> <td>   25.600</td> <td>  118.287</td> <td> 0.000</td> <td> 2976.534</td> <td> 3079.719</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GateDiam</th>     <td> -501.1348</td> <td>  141.084</td> <td>   -3.552</td> <td> 0.001</td> <td> -785.472</td> <td> -216.798</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RiserDiam</th>    <td>   11.4566</td> <td>    1.617</td> <td>    7.084</td> <td> 0.000</td> <td>    8.197</td> <td>   14.716</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>RiserHeight</th>  <td>    7.3724</td> <td>    1.883</td> <td>    3.915</td> <td> 0.000</td> <td>    3.577</td> <td>   11.168</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GateDiamSq</th>   <td>  761.1061</td> <td>  269.944</td> <td>    2.819</td> <td> 0.007</td> <td>  217.069</td> <td> 1305.143</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>GateDiamCube</th> <td> -419.2019</td> <td>  164.496</td> <td>   -2.548</td> <td> 0.014</td> <td> -750.722</td> <td>  -87.682</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 3.914</td> <th>  Durbin-Watson:     </th> <td>   1.731</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.141</td> <th>  Jarque-Bera (JB):  </th> <td>   3.160</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.610</td> <th>  Prob(JB):          </th> <td>   0.206</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.165</td> <th>  Cond. No.          </th> <td>3.74e+03</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.74e+03. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              BatchTime   R-squared:                       0.825\n",
       "Model:                            OLS   Adj. R-squared:                  0.805\n",
       "Method:                 Least Squares   F-statistic:                     41.55\n",
       "Date:                Tue, 06 Apr 2021   Prob (F-statistic):           1.38e-15\n",
       "Time:                        15:57:53   Log-Likelihood:                -154.50\n",
       "No. Observations:                  50   AIC:                             321.0\n",
       "Df Residuals:                      44   BIC:                             332.5\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "================================================================================\n",
       "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------\n",
       "const         3028.1264     25.600    118.287      0.000    2976.534    3079.719\n",
       "GateDiam      -501.1348    141.084     -3.552      0.001    -785.472    -216.798\n",
       "RiserDiam       11.4566      1.617      7.084      0.000       8.197      14.716\n",
       "RiserHeight      7.3724      1.883      3.915      0.000       3.577      11.168\n",
       "GateDiamSq     761.1061    269.944      2.819      0.007     217.069    1305.143\n",
       "GateDiamCube  -419.2019    164.496     -2.548      0.014    -750.722     -87.682\n",
       "==============================================================================\n",
       "Omnibus:                        3.914   Durbin-Watson:                   1.731\n",
       "Prob(Omnibus):                  0.141   Jarque-Bera (JB):                3.160\n",
       "Skew:                          -0.610   Prob(JB):                        0.206\n",
       "Kurtosis:                       3.165   Cond. No.                     3.74e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.74e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minAIC_OLS(X,y):\n",
    "    variables = X.columns\n",
    "    model = sm.OLS(y,X[variables]).fit()\n",
    "    while True:\n",
    "        print(f'old model aic: {model.aic}')\n",
    "        maxp = np.max(model.pvalues)\n",
    "        newvariables = variables[model.pvalues < maxp]\n",
    "        removed = variables[model.pvalues == maxp].values\n",
    "        print(f'considering a model with these variables removed: {removed}')\n",
    "        newmodel = sm.OLS(y,X[newvariables]).fit()\n",
    "        print(f'new model aic: {newmodel.aic}')\n",
    "        if newmodel.aic < model.aic:\n",
    "            model = newmodel\n",
    "            variables = newvariables\n",
    "        else:\n",
    "            break\n",
    "    return model,variables\n",
    "\n",
    "# Split into training and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=2)\n",
    "\n",
    "# now call the minAIC function on our predictors and response variables\n",
    "new_train_linear_model, linear_variables = minAIC_OLS(X_train, y_train)\n",
    "\n",
    "# Now fit the linear model on the new predictors and the test data\n",
    "new_linear_model = sm.OLS(y_test,X_test[linear_variables]).fit()\n",
    "results = new_linear_model.summary()\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fJDA78s_JtD6"
   },
   "source": [
    "## Question 5\n",
    " \n",
    "What model for predicting BatchTime is selected by minAIC_OLS? Are all variables in that model statistically significant at the 95% level under the test data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J2f9qAZ5g9ca"
   },
   "source": [
    "Ans: The model with parameters const, GateDiam, RiserDiam, RiserHeight, GateDiamSq, GateDiamCube. Yes, all variables are at the 95% level under the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nh8iYfCjhIT5"
   },
   "source": [
    "Now we will use the logistic model for `Feasible` and the linear model for `BatchTime`, to inform how to set the values of the mold-variables to minimize expected `BatchTime` and maximize $P(`Feasible = 1)$.\n",
    "\n",
    "## Question 6\n",
    "For each of the mold-variables and each of the outcomes $P(Feasible =1)$ and `BatchTime`, consider increasing the variable by a small amount from its mean value in the full dataset.  State whether this would increase the outcome, decrease it, or have no effect. Base your answers on the models for each response variable selected using AIC and trained on the test data. Include a predictor's estimated effect on an outcome even if its pvalue is not below 0.05.\n",
    "\n",
    "Hint 1: Consider the following plot to when trying to understand the effect of `GateDiam` on `BatchTime`. The plot shows how $GatePoly(X_{GateDiam}) := \\beta_{GateDiam}* X_{GateDiam} + \\beta_{GateDiamSq}*X_{GateDiamSq}+\\beta_{GateDiamCube}* X_{GateDiamCube}$ changes as `GateDiam` increases. \n",
    "\n",
    "Hint 2: The effect of `GateDiam` on `Feasible` is much simpler.  Just look back on your fitted logistic regression results to understand this.  You don't need a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4UuRRHh9jQTg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'GateDiamPoly')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "GateDiamPoly = X[['GateDiam','GateDiamSq','GateDiamCube']].dot(new_linear_model.params[['GateDiam','GateDiamSq','GateDiamCube']])\n",
    "plt.scatter(df['GateDiam'],GateDiamPoly)\n",
    "plt.xlabel('GateDiam')\n",
    "plt.ylabel('GateDiamPoly')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3QngtClnheQ5"
   },
   "source": [
    "Ans:\n",
    "* Bachtime -- (GateDiam, decrease), (RiserDiam, increase), (RiserHeight, increase)\n",
    "* Feasible -- (RiserDiam, increase), (SprueDiamBot, decrease), (SprueDiamTop, decrease), (RiserHeight, decrease)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rM2FhtttpJMU"
   },
   "source": [
    "## Question 7\n",
    "Our goal is to set the mold-variables to minimize expected `BatchTime` and\n",
    "maximize $P(Feasible = 1)$. We will constrain the value of each predictor to be between the minimum and maximum values in `castdata.csv`.\n",
    "\n",
    "Based on your answers to the previous question, for each variable that affected one response variable but not the other, state its optimal value for increasing the probability of feasibility and decreasing batch time. For most (and perhaps all) predictors, the value will either be at the minimum or maximum of the values in `castdata.csv` for that predictor --- in these cases you can just say `min` or `max`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RrzvFcKzpaEH"
   },
   "source": [
    "Ans: \n",
    "* GateDiam, max\n",
    "* SprueDiamBot, min\n",
    "* SprueDiamTop, min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xy1D-IdVpqkK"
   },
   "source": [
    "# Randomized Search for Better Operating Conditions\n",
    "\n",
    "We now search to find values of the mold-variables that give a low expected `BatchTime` subject to the probability that the casting is feasible being at least 0.95. This problem is best solved by linear programming, but, since linear programming is not a prerequisite for this course, we will use a randomized search.\n",
    "\n",
    "Now we create a new set called newdata with 1,000,000 rows where the predictors vary uniformly between their smallest and largest values in the original data. The responses `BatchTime` and `Feasible` are not included in this data set. Rather, the expected values of `BatchTime` and `Feasible` are predicted using the logistic model and the linear model, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "glC_mJ17wTld"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>const</th>\n",
       "      <th>GateDiam</th>\n",
       "      <th>CupHeight</th>\n",
       "      <th>SprueHeight</th>\n",
       "      <th>RiserDiam</th>\n",
       "      <th>SprueDiamBot</th>\n",
       "      <th>SprueDiamTop</th>\n",
       "      <th>RiserHeight</th>\n",
       "      <th>Riser1Pos</th>\n",
       "      <th>Riser2Pos</th>\n",
       "      <th>GateDiamSq</th>\n",
       "      <th>GateDiamCube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.728633</td>\n",
       "      <td>4.782974</td>\n",
       "      <td>11.296558</td>\n",
       "      <td>7.497856</td>\n",
       "      <td>0.749386</td>\n",
       "      <td>1.049829</td>\n",
       "      <td>3.673499</td>\n",
       "      <td>2.972775</td>\n",
       "      <td>5.886284</td>\n",
       "      <td>0.530907</td>\n",
       "      <td>0.386836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.219408</td>\n",
       "      <td>5.575331</td>\n",
       "      <td>11.519561</td>\n",
       "      <td>5.695499</td>\n",
       "      <td>0.646504</td>\n",
       "      <td>0.903803</td>\n",
       "      <td>4.094723</td>\n",
       "      <td>3.045477</td>\n",
       "      <td>5.443165</td>\n",
       "      <td>0.048140</td>\n",
       "      <td>0.010562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.635229</td>\n",
       "      <td>4.803371</td>\n",
       "      <td>10.851808</td>\n",
       "      <td>7.542823</td>\n",
       "      <td>0.708284</td>\n",
       "      <td>1.077576</td>\n",
       "      <td>5.283172</td>\n",
       "      <td>3.675518</td>\n",
       "      <td>4.762267</td>\n",
       "      <td>0.403516</td>\n",
       "      <td>0.256325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.713357</td>\n",
       "      <td>4.583025</td>\n",
       "      <td>10.377216</td>\n",
       "      <td>7.187206</td>\n",
       "      <td>0.534918</td>\n",
       "      <td>0.532767</td>\n",
       "      <td>4.861789</td>\n",
       "      <td>2.550009</td>\n",
       "      <td>5.292597</td>\n",
       "      <td>0.508878</td>\n",
       "      <td>0.363011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.543542</td>\n",
       "      <td>4.154392</td>\n",
       "      <td>9.184616</td>\n",
       "      <td>6.733361</td>\n",
       "      <td>0.731484</td>\n",
       "      <td>0.352599</td>\n",
       "      <td>4.012132</td>\n",
       "      <td>2.574189</td>\n",
       "      <td>5.855998</td>\n",
       "      <td>0.295438</td>\n",
       "      <td>0.160583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   const  GateDiam  CupHeight  SprueHeight  RiserDiam  SprueDiamBot  \\\n",
       "0    1.0  0.728633   4.782974    11.296558   7.497856      0.749386   \n",
       "1    1.0  0.219408   5.575331    11.519561   5.695499      0.646504   \n",
       "2    1.0  0.635229   4.803371    10.851808   7.542823      0.708284   \n",
       "3    1.0  0.713357   4.583025    10.377216   7.187206      0.534918   \n",
       "4    1.0  0.543542   4.154392     9.184616   6.733361      0.731484   \n",
       "\n",
       "   SprueDiamTop  RiserHeight  Riser1Pos  Riser2Pos  GateDiamSq  GateDiamCube  \n",
       "0      1.049829     3.673499   2.972775   5.886284    0.530907      0.386836  \n",
       "1      0.903803     4.094723   3.045477   5.443165    0.048140      0.010562  \n",
       "2      1.077576     5.283172   3.675518   4.762267    0.403516      0.256325  \n",
       "3      0.532767     4.861789   2.550009   5.292597    0.508878      0.363011  \n",
       "4      0.352599     4.012132   2.574189   5.855998    0.295438      0.160583  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upperbounds = df[predictors].max()\n",
    "lowerbounds = df[predictors].min()\n",
    "N = 1000000\n",
    "np.random.seed(10) \n",
    "newdata = pd.DataFrame()\n",
    "for i in range(9):\n",
    "    newdata[df.columns[i]] = np.random.uniform(lowerbounds[i],upperbounds[i],N)\n",
    "\n",
    "newdata = sm.add_constant(newdata)\n",
    "newdata['GateDiamSq']=newdata['GateDiam']**2\n",
    "newdata['GateDiamCube']=newdata['GateDiam']**3\n",
    "newdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fk9jwmYozIKe"
   },
   "source": [
    "Next, use `model.predict()` to compute the probability that `Feasible` is equal to 1, and create a sub-dataset called `newdata_feas` of `newdata` where this probability exceeds 0.95. \n",
    "\n",
    "Note: be careful when using `.predict()` for classification problems. Depending on the package it will give the probability of the outcome being 1 or a prediction (0 or 1). For example when using sklearn.LogisiticRegression, `.predict()` gives a 0-1 prediction and `.predict_prob()` gives the probability of the outcome being 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fA_BbmEuzy8O"
   },
   "outputs": [],
   "source": [
    "#predict probability of feasible block for each data point and remove points \n",
    "#with probability less than 0.95\n",
    "Yhat = new_model.predict(newdata[logit_variables])\n",
    "newdata_feas = newdata[pd.Series(Yhat >= 0.95)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zx_XGXoh2RPu"
   },
   "source": [
    "Now, predict the value of `BatchTime` for all rows of newdata_feas and find the row where `BatchTime` is minimized; this is the single row of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SrIGR_2-3xu2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2979.1418288158625\n"
     ]
    }
   ],
   "source": [
    "exBatchTime = new_linear_model.predict(newdata_feas[linear_variables])\n",
    "print(exBatchTime.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3MW-8AwHESgY"
   },
   "source": [
    "## Question 9\n",
    "What is the expected value of BatchTime under optimal conditions? “Optimal\n",
    "conditions” means that the predictors are set to the best feasible values in newdata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fz7gLML_15oY"
   },
   "source": [
    "Ans: 2979.14 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help you answer the next question, here are the minimum and maximum value of each predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GateDiam        0.205329\n",
       "CupHeight       3.947455\n",
       "SprueHeight     8.848969\n",
       "RiserDiam       5.417082\n",
       "SprueDiamBot    0.256138\n",
       "SprueDiamTop    0.321120\n",
       "RiserHeight     3.399719\n",
       "Riser1Pos       2.535606\n",
       "Riser2Pos       3.629352\n",
       "dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[predictors].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GateDiam         0.883781\n",
       "CupHeight        5.999328\n",
       "SprueHeight     11.877841\n",
       "RiserDiam        8.303038\n",
       "SprueDiamBot     0.849809\n",
       "SprueDiamTop     1.231756\n",
       "RiserHeight      5.978733\n",
       "Riser1Pos        4.387350\n",
       "Riser2Pos        6.010920\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[predictors].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Huo3hg0b1goU"
   },
   "source": [
    "## Question 10\n",
    "What are the “optimal” values of the variables, where “optimal” means minimizing the expected `BatchTime` subject to $P(Feasible = 1) > 0.95$. Compare these results with your answer to Question 7 and say whether your results are consistent. You may also want to output the maximum and minimum values for each variable using the code `df.max()` and `df.min()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dm3NUmCoEkzk"
   },
   "source": [
    "Ans: These answers are partially consistant -- the 'optimal' GateDiam value is the max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CD4ThqzCFaS5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "const            1.000000\n",
      "GateDiam         0.846901\n",
      "CupHeight        4.644741\n",
      "SprueHeight     10.849084\n",
      "RiserDiam        8.032133\n",
      "SprueDiamBot     0.710432\n",
      "SprueDiamTop     0.881679\n",
      "RiserHeight      4.453623\n",
      "Riser1Pos        3.619079\n",
      "Riser2Pos        5.209221\n",
      "GateDiamSq       0.717242\n",
      "GateDiamCube     0.607433\n",
      "Name: 955082, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "print(newdata_feas.iloc[np.argmin(exBatchTime)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RFBXBMHCEjaQ"
   },
   "source": [
    "## Question 11\n",
    "\n",
    "What is the probability that Feasible will equal 1 under optimal conditions?\n",
    "Hint: Use predict again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "abOA8aeJFk_H"
   },
   "source": [
    "Ans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.95939371])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.predict(newdata_feas.iloc[np.argmin(exBatchTime)][logit_variables].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~96% probability"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Recitation_11– Logistic_Regression.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
